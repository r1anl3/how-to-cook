# Ôn tập NT533 - Hệ tính toán phân bố

## Week 1 + 2: Course Introduction, Fundamentals

### Some keywords from the 5th Computer System Generation

- *The network is the computer*
- Distributed system: **Cluster, Cloud, Grid, P2P-Computing**
- Multicore processors and **parallel applications**
- Virtualization: **VMware, [XEN](https://en.wikipedia.org/wiki/Xen), KVM (Kernel-based Virtual Machine), Docker**
- Open Source: **Linux**, [BSD](https://people.freebsd.org/~pizzamig/nomad-pot-DevSummit.pdf)

### Client - Server

A system includes

- One or more **client** which use the services of the server and accesses data, stored at the server (⇒ consumers)
- A **Server** which provides services and/or data (⇒ producer)

The connection establishment is initiated by the clients

Communication works according to a protocol

- A client sends a request to the server 
- The server responds with a reply

The client-server architecture consists of 2 layers and is called two-tier model (tier = layer)

Tasks in Client - Server Model

- Display (graphical) user interface
- Calculation of the (graphical) user interface
- Data processing
- Data management
- Data storage

Types of Clients

- Text-Terminals, X-Terminals
	- Only display the (graphical) user interface and transfer the user interaction to the server
	- Calculation of the (graphical) user interface, data processing and data storage, data management are tasks of the server
- Thin/Zero Clients
	- Calculate and display the graphical user interface
- Applet Clients
	- Calculate and display the graphical user interface and do a part of the data processing
	- The clients process the applications (applets) themselves
- Fat Clients
	- Only data management and data storage are located on the (file or database) server

Advantages of Thin Clients

- Low acquisition cost
- Reduced power consumption => reduced operating costs
- Little space consumption
- Reduced noise
- Central storage of data is more efficient and more secure
- Virtualization on the server
- Reduced effort for administration

Drawback of Thin Clients

- No 3D graphics performance
- Limited extensibility
- Users fear storing their data outside of their PC
- Server is a single point of failure and eventually a bottleneck

### Moore's Law

Published in 1965 by Gordon Moore

It is based of empirical observation

Moore originally meant the electronic components on of integrated circuit double every 12 months

Since the late 1970s, the packing density only doubles every 24 months

### Von Neumann Bottleneck

The data and control bus is increasingly becoming a bottleneck between the CPU and memory

The main memory and the bus system are key factors for the performance of a computer

The Von Neumann Architecture describes the structure of the general-purpose computer, which is not limited to a fixed program and has input and output devices

Main difference to modern systems: A single Bus to connect I/O devices directly with the CPU, is impossible today

Main memory is usually DRAM (Dynamic Random Access Memory)

The access time (or cycle time)

  - DDR-400 SDRAM is 5 ns: 200 MHz
  - DDR3-2400 SDRAM is 0.833 ns: 1200 MHz
  - DDR4-4800 SDRAM is 0.417 ns: 2400
  
> *Note: 1 Hz = $\frac{1}{seconds}$*  

To reduce bottleneck impact

- Caches is SRAM (Static Random Access Memory), its access speed is close to the CPU speed, and it can reduce the bottleneck impact

To increase bottleneck impact

- If multiple CPUs (or cores) share the main memory and thus share the memory bus 

### Amdahl's Law

Published in 1967, named after Gene Myron Amdahl

Calculates the maximum expected acceleration of programs by parallel execution on multiple CPUs

According to Amdahl, the performance gain is limited mainly by the sequential part of the problem

A program can never be fully executed in parallel

Formula: $S = \frac{1}{1 - P + \frac{P}{N}}$

> Source: [Computer Organization | Amdahl's law and its proof - GeeksforGeeks](https://www.geeksforgeeks.org/computer-organization-amdahls-law-and-its-proof/)

- S: speedup of the system
- P: proportion of the system that can be improved
- N: number of processors in the system

(Example):  If a system has a single bottleneck that occupies 20% of the total execution time, and we add 4 more processors to the system, the speedup would be:

- $S = \frac{1}{1 - 0.2 + \frac{0.2}{4 + 1}}$ 
- S = 1.19
- This means that the overall performance of the system would improve by about 19% with the addition of the 4 processors.

Issues

- Amdahl’s law does not take into account the cache and the effects, which are caused by the cache in practice
- In the optimal case, the entire data can be stored in the cache. In such a case (very rare), a super-linear SpeedUp may occur: $S_(p) = \frac{t_(s)}{t_(p)}$ , with
	- $S_(p):$ Speedup Factor when using p CPU cores of a multiprocessor system
	- $t_(s):$ execution time by using a single CPU core
	- $t_(p):$ execution time by using p CPU cores
- The max. SpeedUp is usually p with p CPU cores (=⇒ linear SpeedUp)
- A super-linear SpeedUp is greater than p

Transferred to parallel computers, this means that with a growing number of CPUs, the problem size should grow too

The problem needs to scale with the number of CPUs

### Gustafson's Law

Gustafson’s Law from John Gustafson (1988) says that a problem, which is sufficiently large, can be parallelized efficiently

Difference to Amdahl’s law

- The parallel portion of the problem grows with the number of CPUs
- The sequential part is not limiting, because it gets more and more unimportant as the number of CPUs rises

Formula: $\text{scaled speedup}= N +(1−N)s$ 

> Source: [When Should I Use Parallel Programming? Gustafson’s Law | Medium](https://medium.com/@kelvinmuchiri/when-should-i-use-parallel-programming-gustafsons-law-in-action-9dffd728708b)

- N: number of processors in the system
- s: the fraction of the computation that is serial (i.e., not parallelizable)

> *Note: In some documents, s is replaced with $\alpha$* 

(Example): Suppose we have a program that is **70% parallel** and **30% sequential**, and we have **10 processors**

$\text{Scaled speedup} = N +(1−N)s  = 10 + (1–10) \times 0.3  = 7.3$

> [!NOTE]  
> Due to [Gustafson's law - Wikipedia](https://en.wikipedia.org/wiki/Gustafson%27s_law), s is the fraction time spent executing the serial parts, and p is the fraction time for parallel parts of the program. If the given fraction is s, then use the above formula, else use this formula: $S = 1 + (N - 1) \times p$ 

### Parallel Computers


## Week 3 + 4: Cluster Computing, FaaS, Container, Function as a Service

## Week 5 + 6: Cloud Computing, Services and Concepts

## Week 7 + 8: Infrastructure Services (IaaS), Platform Services (PaaS)

## Week 9 - 10: Hadoop - MapReduce - HBase